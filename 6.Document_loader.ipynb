{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load from PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local file or online file \n",
    "# 16 Pages\n",
    "url = \"https://arxiv.org/pdf/2312.16862.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Load per page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "pdf_loader = PyPDFLoader(url)\n",
    "docs = pdf_loader.load()\n",
    "len(docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Published as a conference paper at COLM 2024\\nTinyGPT-V: Efficient Multimodal Large Language Model\\nvia Small Backbones\\nZhengqing Yuan1, Zhaoxu Li2∗, Weiran Huang3, Yanfang Ye1, Lichao Sun2†\\n1University of Notre Dame2Lehigh University3Shanghai Jiao Tong University\\nAbstract\\nIn recent years, multimodal large language models (MLLMs) such as GPT-\\n4V have demonstrated remarkable advancements, excelling in a variety\\nof vision-language tasks. Despite their prowess, the closed-source na-\\nture and computational demands of such models limit their accessibility\\nand applicability. This study introduces TinyGPT-V , a novel open-source\\nMLLM, designed for efficient training and inference across various vision-\\nlanguage tasks, including image captioning (IC) and visual question an-\\nswering (VQA). Leveraging a compact yet powerful architecture, TinyGPT-\\nV integrates the Phi-2 language model with pre-trained vision encoders,\\nutilizing a unique mapping module for visual and linguistic information\\nfusion. With a training regimen optimized for small backbones and em-\\nploying a diverse dataset amalgam, TinyGPT-V requires significantly lower\\ncomputational resources—24GB for training and as little as 8GB for infer-\\nence—without compromising on performance. Our experiments demon-\\nstrate that TinyGPT-V , with its language model 2.8 billion parameters,\\nachieves comparable results in VQA and image inference tasks to its larger\\ncounterparts while being uniquely suited for deployment on resource-\\nconstrained devices through innovative quantization techniques. This\\nwork not only paves the way for more accessible and efficient MLLMs but\\nalso underscores the potential of smaller, optimized models in bridging the\\ngap between high performance and computational efficiency in real-world\\napplications. Additionally, this paper introduces a new approach to multi-\\nmodal large language models using smaller backbones. Our code and train-\\ning weights are available in https://github.com/DLYuanGod/TinyGPT-V .\\n1 Introduction\\nIn recent years, the field of artificial intelligence has seen significant advancements through\\nthe development of multimodal large language models (MLLMs), such as GPT-4V , which\\nhave shown exceptional performance across a range of vision-language tasks (Yang\\net al., 2023). Despite GPT-4V’s impressive capabilities, its closed-source nature limits\\nits widespread application and adaptability. In contrast, the open-source landscape for\\nMLLMs is rapidly evolving, presenting models like LLaVA and MiniGPT-4 that excel in\\nimage captioning (IC), visual question answering (VQA) often comparable GPT-4V in these\\nareas (Dai et al., 2023; Liu et al., 2023a;b; Zhu et al., 2023). Notably, MiniGPT-v2 (Chen et al.,\\n2023) has demonstrated superior performance in various visual grounding and question-\\nanswering tasks. However, its training code remains proprietary, which poses challenges\\nfor community-driven advancements and adaptability.\\nAlthough the impressive vision-language capabilities demonstrated by some open-source\\nMLLMs, they frequently necessitate significant computational resources for training and\\ninference. For example, training LLaVA-v1.5-13B (Liu et al., 2023a) required 8 ×A100 GPUs,\\neach equipped with 80GB of memory, cumulating in 25.5 hours of continuous training. As\\nshown in Figure 1 (a), the underlying performance of large language models, which are\\n*Zhaoxu is a visiting student in the LAIR lab at Lehigh University.\\n†Lichao Sun is co-corresponding author: lis221@lehigh.edu\\n1arXiv:2312.16862v2  [cs.CV]  4 Apr 2024', metadata={'source': 'https://arxiv.org/pdf/2312.16862.pdf', 'page': 0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Extract image as text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install rapidocr-onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_loader = PyPDFLoader(url, extract_images=True)\n",
    "docs = pdf_loader.load()\n",
    "len(docs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Load UnStructured PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PDFMinerLoader\n",
    "pdf_loader = PDFMinerLoader(url)\n",
    "docs = pdf_loader.load()\n",
    "len(docs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Load PDF from dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "path_dir = \"./data_source\"\n",
    "pdf_loader = PyPDFDirectoryLoader(path_dir)\n",
    "docs = pdf_loader.load()\n",
    "len(docs) # 36 pages from 2 pdf files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load from HTML file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load UnStructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"What I learned from looking at 900 most popular open source AI tools\\n\\nMar 14, 2024\\n      \\n      \\n        • Chip Huyen\\n\\n[Hacker News discussion, LinkedIn discussion, Twitter thread]\\n\\nFour years ago, I did an analysis of the open source ML ecosystem. Since then, the landscape has changed, so I revisited the topic. This time, I focused exclusively on the stack around foundation models.\\n\\nThe full list of open source AI repos is hosted at llama-police. The list is updated every 6 hours. You can also find most of them on my cool-llm-repos list on GitHub.\\n\\nData\\n\\n….\\n\\nHow to add missing repos\\n\\nThe New AI Stack\\n\\n….\\n\\nAI stack over time\\n\\n……..\\n\\nApplications\\n\\n……..\\n\\nAI engineering\\n\\n……..\\n\\nModel development\\n\\n……..\\n\\nInfrastructure\\n\\nOpen source AI developers\\n\\n….\\n\\nOne-person billion-dollar companies?\\n\\n….\\n\\n1 million commits\\n\\nThe growing China’s open source ecosystem\\n\\nLive fast, die young\\n\\nMy personal favorite ideas\\n\\nConclusion\\n\\nData\\n\\nI searched GitHub using the keywords gpt, llm, and generative ai. If AI feels so overwhelming right now, it’s because it is. There are 118K results for gpt alone.\\n\\nTo make my life easier, I limited my search to the repos with at least 500 stars. There were 590 results for llm, 531 for gpt, and 38 for generative ai. I also occasionally checked GitHub trending and social media for new repos.\\n\\nAfter MANY hours, I found 896 repos. Of these, 51 are tutorials (e.g. dair-ai/Prompt-Engineering-Guide) and aggregated lists (e.g. f/awesome-chatgpt-prompts). While these tutorials and lists are helpful, I’m more interested in software. I still include them in the final list, but the analysis is done with the 845 software repositories.\\n\\nIt was a painful but rewarding process. It gave me a much better understanding of what people are working on, how incredibly collaborative the open source community is, and just how much China’s open source ecosystem diverges from the Western one.\\n\\nAdd missing repos\\n\\nI undoubtedly missed a ton of repos. You can submit the missing repos here. The list will be automatically updated every day.\\n\\nFeel free to submit the repos with less than 500 stars. I’ll continue tracking them and add them to the list when they reach 500 stars!\\n\\nThe New AI Stack\\n\\nI think of the AI stack as consisting of 4 layers: infrastructure, model development, application development, and applications.\\n\\nInfrastructure\\n\\n    At the bottom is the stack is infrastructure, which includes toolings for serving (vllm, NVIDIA’s Triton), compute management (skypilot), vector search and database (faiss, milvus, qdrant, lancedb), ….\\n\\nModel development\\n\\n    This layer provides toolings for developing models, including frameworks for modeling & training (transformers, pytorch, DeepSpeed), inference optimization (ggml, openai/triton), dataset engineering, evaluation, ….. Anything that involves changing a model’s weights happens in this layer, including finetuning.\\n\\nApplication development\\n With readily available models, anyone can develop applications on top of them. This is the layer that has seen the most actions in the last 2 years and is still rapidly evolving. This layer is also known as AI engineering.\\n\\n    Application development involves prompt engineering, RAG, AI interface, …\\n\\nApplications\\n\\n    There are many open sourced applications built on top of existing models. The most popular types of applications are coding, workflow automation, information aggregation, …\\n\\nOutside of these 4 layers, I also have another category, Model repos, that are created by companies and researchers to share the code associated with their models. Examples of repos in this category are CompVis/stable-diffusion, openai/whisper, and facebookresearch/llama.\\n\\nAI stack over time\\n\\nI plotted the cumulative number of repos in each category month-over-month. There was an explosion of new toolings in 2023, after the introduction of Stable Diffusion and ChatGPT. The curve seems to flatten in September 2023 because of three potential reasons.\\n\\nI only include repos with at least 500 stars in my analysis, and it takes time for repos to gather these many stars.\\n\\nMost low-hanging fruits have been picked. What is left takes more effort to build, hence fewer people can build them.\\n\\nPeople have realized that it’s hard to be competitive in the generative AI space, so the excitement has calmed down. Anecdotally, in early 2023, all AI conversations I had with companies centered around gen AI, but the recent conversations are more grounded. Several even brought up scikit-learn. I’d like to revisit this in a few months to verify if it’s true.\\n\\nIn 2023, the layers that saw the highest increases were the applications and application development layers. The infrastructure layer saw a little bit of growth, but it was far from the level of growth seen in other layers.\\n\\nApplications\\n\\nNot surprisingly, the most popular types of applications are coding, bots (e.g. role-playing, WhatsApp bots, Slack bots), and information aggregation (e.g. “let’s connect this to our Slack and ask it to summarize the messages each day”).\\n\\nAI engineering\\n\\n2023 was the year of AI engineering. Since many of them are similar, it’s hard to categorize the tools. I currently put them into the following categories: prompt engineering, AI interface, Agent, and AI engineering (AIE) framework.\\n\\nPrompt engineering goes way beyond fiddling with prompts to cover things like constrained sampling (structured outputs), long-term memory management, prompt testing & evaluation, etc.\\n\\nAI interface provides an interface for your end users to interact with your AI application. This is the category I’m the most excited about. Some of the interfaces that are gaining popularity are:\\n\\nWeb and desktop apps.\\n\\nBrowser extensions that let users quickly query AI models while browsing.\\n\\nBots via chat apps like Slack, Discord, WeChat, and WhatsApp.\\n\\nPlugins that let developers embed AI applications to applications like VSCode, Shopify, and Microsoft Offices. The plugin approach is common for AI applications that can use tools to complete complex tasks (agents).\\n\\nAIE framework is a catch-all term for all platforms that help you develop AI applications. Many of them are built around RAG, but many also provide other toolings such as monitoring, evaluation, etc.\\n\\nAgent is a weird category, as many agent toolings are just sophisticated prompt engineering with potentially constrained generation (e.g. the model can only output the predetermined action) and plugin integration (e.g. to let the agent use tools).\\n\\nModel development\\n\\nPre-ChatGPT, the AI stack was dominated by model development. Model development’s biggest growth in 2023 came from increasing interest in inference optimization, evaluation, and parameter-efficient finetuning (which is grouped under Modeling & training).\\n\\nInference optimization has always been important, but the scale of foundation models today makes it crucial for latency and cost. The core approaches for optimization remain the same (quantization, low-ranked factorization, pruning, distillation), but many new techniques have been developed especially for the transformer architecture and the new generation of hardware. For example, in 2020, 16-bit quantization was considered state-of-the-art. Today, we’re seeing 2-bit quantization and even lower than 2-bit.\\n\\nSimilarly, evaluation has always been essential, but with many people today treating models as blackboxes, evaluation has become even more so. There are many new evaluation benchmarks and evaluation methods, such as comparative evaluation (see Chatbot Arena) and AI-as-a-judge.\\n\\nInfrastructure\\n\\nInfrastructure is about managing data, compute, and toolings for serving, monitoring, and other platform work. Despite all the changes that generative AI brought, the open source AI infrastructure layer remained more or less the same. This could also be because infrastructure products are typically not open sourced.\\n\\nThe newest category in this layer is vector database with companies like Qdrant, Pinecone, and LanceDB. However, many argue this shouldn’t be a category at all. Vector search has been around for a long time. Instead of building new databases just for vector search, existing database companies like DataStax and Redis are bringing vector search into where the data already is.\\n\\nOpen source AI developers\\n\\nOpen source software, like many things, follows the long tail distribution. A handful of accounts control a large portion of the repos.\\n\\nOne-person billion-dollar companies?\\n\\n845 repos are hosted on 594 unique GitHub accounts. There are 20 accounts with at least 4 repos. These top 20 accounts host 195 of the repos, or 23% of all the repos on the list. These 195 repos have gained a total of 1,650,000 stars.\\n\\nOn Github, an account can be either an organization or an individual. 19/20 of the top accounts are organizations. Of those, 3 belong to Google: google-research, google, tensorflow.\\n\\nThe only individual account in these top 20 accounts is lucidrains. Among the top 20 accounts with the most number of stars (counting only gen AI repos), 4 are individual accounts:\\n\\nlucidrains (Phil Wang): who can implement state-of-the-art models insanely fast.\\n\\nggerganov (Georgi Gerganov): an optimization god who comes from a physics background.\\n\\nIllyasviel (Lyumin Zhang): creator of Foocus and ControlNet who’s currently a Stanford PhD.\\n\\nxtekky: a full-stack developer who created gpt4free.\\n\\nUnsurprisingly, the lower we go in the stack, the harder it is for individuals to build. Software in the infrastructure layer is the least likely to be started and hosted by individual accounts, whereas more than half of the applications are hosted by individuals.\\n\\nApplications started by individuals, on average, have gained more stars than applications started by organizations. Several people have speculated that we’ll see many very valuable one-person companies (see Sam Altman’s interview and Reddit discussion). I think they might be right.\\n\\n1 million commits\\n\\nOver 20,000 developers have contributed to these 845 repos. In total, they’ve made almost a million contributions!\\n\\nAmong them, the 50 most active developers have made over 100,000 commits, averaging over 2,000 commits each. See the full list of the top 50 most active open source developers here.\\n\\nThe growing China's open source ecosystem\\n\\nIt’s been known for a long time that China’s AI ecosystem has diverged from the US (I also mentioned that in a 2020 blog post). At that time, I was under the impression that GitHub wasn’t widely used in China, and my view back then was perhaps colored by China’s 2013 ban on GitHub.\\n\\nHowever, this impression is no longer true. There are many, many popular AI repos on GitHub targeting Chinese audiences, such that their descriptions are written in Chinese. There are repos for models developed for Chinese or Chinese + English, such as Qwen, ChatGLM3, Chinese-LLaMA.\\n\\nWhile in the US, many research labs have moved away from the RNN architecture for language models, the RNN-based model family RWKV is still popular.\\n\\nThere are also AI engineering tools providing ways to integrate AI models into products popular in China like WeChat, QQ, DingTalk, etc. Many popular prompt engineering tools also have mirrors in Chinese.\\n\\nAmong the top 20 accounts on GitHub, 6 originated in China:\\n\\nTHUDM: Knowledge Engineering Group (KEG) & Data Mining at Tsinghua University.\\n\\nOpenGVLab: General Vision team of Shanghai AI Laboratory\\n\\nOpenBMB: Open Lab for Big Model Base, founded by ModelBest & the NLP group at Tsinghua University.\\n\\nInternLM: from Shanghai AI Laboratory.\\n\\nOpenMMLab: from The Chinese University of Hong Kong.\\n\\nQwenLM: Alibaba’s AI lab, which publishes the Qwen model family.\\n\\nLive fast, die young\\n\\nOne pattern that I saw last year is that many repos quickly gained a massive amount of eyeballs, then quickly died down. Some of my friends call this the “hype curve”. Out of these 845 repos with at least 500 GitHub stars, 158 repos (18.8%) haven’t gained any new stars in the last 24 hours, and 37 repos (4.5%) haven’t gained any new stars in the last week.\\n\\nHere are examples of the growth trajectory of two of such repos compared to the growth curve of two more sustained software. Even though these two examples shown here are no longer used, I think they were valuable in showing the community what was possible, and it was cool that the authors were able to get things out so fast.\\n\\nMy personal favorite ideas\\n\\nSo many cool ideas are being developed by the community. Here are some of my favorites.\\n\\nBatch inference optimization: FlexGen, llama.cpp\\n\\nFaster decoder with techniques such as Medusa, LookaheadDecoding\\n\\nModel merging: mergekit\\n\\nConstrained sampling: outlines, guidance, SGLang\\n\\nSeemingly niche tools that solve one problem really well, such as einops and safetensors.\\n\\nConclusion\\n\\nEven though I included only 845 repos in my analysis, I went through several thousands of repos. I found this helpful for me to get a big-picture view of the seemingly overwhelming AI ecosystem. I hope the list is useful for you too. Please do let me know what repos I’m missing, and I’ll add them to the list!\\n\\ncomments powered by Disqus.\", metadata={'source': './data_source/ai-oss.html'})]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "html_path = \"./data_source/ai-oss.html\"\n",
    "\n",
    "html_loader = UnstructuredHTMLLoader(html_path)\n",
    "docs = html_loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': './data_source/ai-oss.html'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Load with BS4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"\\n\\n\\n\\n\\nWhat I learned from looking at 900 most popular open source AI tools\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat I learned from looking at 900 most popular open source AI tools | Chip Huyen\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nChip Huyen\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBlog\\nBooks\\nList 100\\nLlama Police\\nMLOps Guide\\nTiếng Việt\\n\\n\\n\\n\\n\\n\\n\\n\\nWhat I learned from looking at 900 most popular open source AI tools\\n\\n\\n        \\n        Mar 14, 2024\\n      \\n      \\n        • Chip Huyen\\n\\n\\n\\n[Hacker News discussion, LinkedIn discussion, Twitter thread]\\nFour years ago, I did an analysis of the open source ML ecosystem. Since then, the landscape has changed, so I revisited the topic. This time, I focused exclusively on the stack around foundation models.\\nThe full list of open source AI repos is hosted at llama-police. The list is updated every 6 hours. You can also find most of them on my cool-llm-repos list on GitHub.\\n\\nTable of contents\\nData\\n…. How to add missing repos\\nThe New AI Stack\\n…. AI stack over time\\n…….. Applications\\n…….. AI engineering\\n…….. Model development\\n…….. Infrastructure\\nOpen source AI developers\\n…. One-person billion-dollar companies?\\n…. 1 million commits\\nThe growing China’s open source ecosystem\\nLive fast, die young\\nMy personal favorite ideas\\nConclusion\\n\\n\\nData\\nI searched GitHub using the keywords gpt, llm, and generative ai. If AI feels so overwhelming right now, it’s because it is. There are 118K results for gpt alone.\\nTo make my life easier, I limited my search to the repos with at least 500 stars. There were 590 results for llm, 531 for gpt, and 38 for generative ai. I also occasionally checked GitHub trending and social media for new repos.\\nAfter MANY hours, I found 896 repos. Of these, 51 are tutorials (e.g. dair-ai/Prompt-Engineering-Guide) and aggregated lists (e.g. f/awesome-chatgpt-prompts). While these tutorials and lists are helpful, I’m more interested in software. I still include them in the final list, but the analysis is done with the 845 software repositories.\\nIt was a painful but rewarding process. It gave me a much better understanding of what people are working on, how incredibly collaborative the open source community is, and just how much China’s open source ecosystem diverges from the Western one.\\nAdd missing repos\\nI undoubtedly missed a ton of repos. You can submit the missing repos here. The list will be automatically updated every day.\\nFeel free to submit the repos with less than 500 stars. I’ll continue tracking them and add them to the list when they reach 500 stars!\\nThe New AI Stack\\nI think of the AI stack as consisting of 4 layers: infrastructure, model development, application development, and applications.\\n\\n\\n\\n\\n\\n\\n\\n\\nInfrastructure\\nAt the bottom is the stack is infrastructure, which includes toolings for serving (vllm, NVIDIA’s Triton), compute management (skypilot), vector search and database (faiss, milvus, qdrant, lancedb), ….\\n\\n\\nModel development\\nThis layer provides toolings for developing models, including frameworks for modeling & training (transformers, pytorch, DeepSpeed), inference optimization (ggml, openai/triton), dataset engineering, evaluation, ….. Anything that involves changing a model’s weights happens in this layer, including finetuning.\\n\\n\\nApplication development\\n With readily available models, anyone can develop applications on top of them. This is the layer that has seen the most actions in the last 2 years and is still rapidly evolving. This layer is also known as AI engineering.\\nApplication development involves prompt engineering, RAG, AI interface, …\\n\\n\\nApplications\\nThere are many open sourced applications built on top of existing models. The most popular types of applications are coding, workflow automation, information aggregation, …\\n\\n\\nOutside of these 4 layers, I also have another category, Model repos, that are created by companies and researchers to share the code associated with their models. Examples of repos in this category are CompVis/stable-diffusion, openai/whisper, and facebookresearch/llama.\\nAI stack over time\\nI plotted the cumulative number of repos in each category month-over-month. There was an explosion of new toolings in 2023, after the introduction of Stable Diffusion and ChatGPT. The curve seems to flatten in September 2023 because of three potential reasons.\\n\\nI only include repos with at least 500 stars in my analysis, and it takes time for repos to gather these many stars.\\nMost low-hanging fruits have been picked. What is left takes more effort to build, hence fewer people can build them.\\nPeople have realized that it’s hard to be competitive in the generative AI space, so the excitement has calmed down. Anecdotally, in early 2023, all AI conversations I had with companies centered around gen AI, but the recent conversations are more grounded. Several even brought up scikit-learn. I’d like to revisit this in a few months to verify if it’s true.\\n\\n\\n\\n\\n\\n\\n\\nIn 2023, the layers that saw the highest increases were the applications and application development layers. The infrastructure layer saw a little bit of growth, but it was far from the level of growth seen in other layers.\\nApplications\\nNot surprisingly, the most popular types of applications are coding, bots (e.g. role-playing, WhatsApp bots, Slack bots), and information aggregation (e.g. “let’s connect this to our Slack and ask it to summarize the messages each day”).\\n\\n\\n\\n\\n\\n\\nAI engineering\\n2023 was the year of AI engineering. Since many of them are similar, it’s hard to categorize the tools. I currently put them into the following categories: prompt engineering, AI interface, Agent, and AI engineering (AIE) framework.\\nPrompt engineering goes way beyond fiddling with prompts to cover things like constrained sampling (structured outputs), long-term memory management, prompt testing & evaluation, etc.\\n\\n\\n\\n\\n\\n\\nAI interface provides an interface for your end users to interact with your AI application. This is the category I’m the most excited about. Some of the interfaces that are gaining popularity are:\\n\\nWeb and desktop apps.\\nBrowser extensions that let users quickly query AI models while browsing.\\nBots via chat apps like Slack, Discord, WeChat, and WhatsApp.\\nPlugins that let developers embed AI applications to applications like VSCode, Shopify, and Microsoft Offices. The plugin approach is common for AI applications that can use tools to complete complex tasks (agents).\\n\\nAIE framework is a catch-all term for all platforms that help you develop AI applications. Many of them are built around RAG, but many also provide other toolings such as monitoring, evaluation, etc.\\nAgent is a weird category, as many agent toolings are just sophisticated prompt engineering with potentially constrained generation (e.g. the model can only output the predetermined action) and plugin integration (e.g. to let the agent use tools).\\n\\n\\n\\n\\n\\n\\nModel development\\nPre-ChatGPT, the AI stack was dominated by model development. Model development’s biggest growth in 2023 came from increasing interest in inference optimization, evaluation, and parameter-efficient finetuning (which is grouped under Modeling & training).\\nInference optimization has always been important, but the scale of foundation models today makes it crucial for latency and cost. The core approaches for optimization remain the same (quantization, low-ranked factorization, pruning, distillation), but many new techniques have been developed especially for the transformer architecture and the new generation of hardware. For example, in 2020, 16-bit quantization was considered state-of-the-art. Today, we’re seeing 2-bit quantization and even lower than 2-bit.\\nSimilarly, evaluation has always been essential, but with many people today treating models as blackboxes, evaluation has become even more so. There are many new evaluation benchmarks and evaluation methods, such as comparative evaluation (see Chatbot Arena) and AI-as-a-judge.\\n\\n\\n\\n\\n\\n\\nInfrastructure\\nInfrastructure is about managing data, compute, and toolings for serving, monitoring, and other platform work. Despite all the changes that generative AI brought, the open source AI infrastructure layer remained more or less the same. This could also be because infrastructure products are typically not open sourced.\\nThe newest category in this layer is vector database with companies like Qdrant, Pinecone, and LanceDB. However, many argue this shouldn’t be a category at all. Vector search has been around for a long time. Instead of building new databases just for vector search, existing database companies like DataStax and Redis are bringing vector search into where the data already is.\\nOpen source AI developers\\nOpen source software, like many things, follows the long tail distribution. A handful of accounts control a large portion of the repos.\\nOne-person billion-dollar companies?\\n845 repos are hosted on 594 unique GitHub accounts. There are 20 accounts with at least 4 repos. These top 20 accounts host 195 of the repos, or 23% of all the repos on the list. These 195 repos have gained a total of 1,650,000 stars.\\n\\n\\n\\n\\n\\n\\nOn Github, an account can be either an organization or an individual. 19/20 of the top accounts are organizations. Of those, 3 belong to Google: google-research, google, tensorflow.\\nThe only individual account in these top 20 accounts is lucidrains. Among the top 20 accounts with the most number of stars (counting only gen AI repos), 4 are individual accounts:\\n\\nlucidrains (Phil Wang): who can implement state-of-the-art models insanely fast.\\nggerganov (Georgi Gerganov): an optimization god who comes from a physics background.\\nIllyasviel (Lyumin Zhang): creator of Foocus and ControlNet who’s currently a Stanford PhD.\\nxtekky: a full-stack developer who created gpt4free.\\n\\n\\n\\n\\n\\n\\n\\nUnsurprisingly, the lower we go in the stack, the harder it is for individuals to build. Software in the infrastructure layer is the least likely to be started and hosted by individual accounts, whereas more than half of the applications are hosted by individuals.\\n\\n\\n\\n\\n\\n\\nApplications started by individuals, on average, have gained more stars than applications started by organizations. Several people have speculated that we’ll see many very valuable one-person companies (see Sam Altman’s interview and Reddit discussion). I think they might be right.\\n\\n\\n\\n\\n\\n\\n1 million commits\\nOver 20,000 developers have contributed to these 845 repos. In total, they’ve made almost a million contributions!\\nAmong them, the 50 most active developers have made over 100,000 commits, averaging over 2,000 commits each. See the full list of the top 50 most active open source developers here.\\n\\n\\n\\n\\n\\n\\nThe growing China's open source ecosystem\\nIt’s been known for a long time that China’s AI ecosystem has diverged from the US (I also mentioned that in a 2020 blog post). At that time, I was under the impression that GitHub wasn’t widely used in China, and my view back then was perhaps colored by China’s 2013 ban on GitHub.\\nHowever, this impression is no longer true. There are many, many popular AI repos on GitHub targeting Chinese audiences, such that their descriptions are written in Chinese. There are repos for models developed for Chinese or Chinese + English, such as Qwen, ChatGLM3, Chinese-LLaMA.\\nWhile in the US, many research labs have moved away from the RNN architecture for language models, the RNN-based model family RWKV is still popular.\\nThere are also AI engineering tools providing ways to integrate AI models into products popular in China like WeChat, QQ, DingTalk, etc. Many popular prompt engineering tools also have mirrors in Chinese.\\nAmong the top 20 accounts on GitHub, 6 originated in China:\\n\\nTHUDM: Knowledge Engineering Group (KEG) & Data Mining at Tsinghua University.\\nOpenGVLab: General Vision team of Shanghai AI Laboratory\\nOpenBMB: Open Lab for Big Model Base, founded by ModelBest & the NLP group at Tsinghua University.\\nInternLM: from Shanghai AI Laboratory.\\nOpenMMLab: from The Chinese University of Hong Kong.\\nQwenLM: Alibaba’s AI lab, which publishes the Qwen model family.\\n\\nLive fast, die young\\nOne pattern that I saw last year is that many repos quickly gained a massive amount of eyeballs, then quickly died down. Some of my friends call this the “hype curve”. Out of these 845 repos with at least 500 GitHub stars, 158 repos (18.8%) haven’t gained any new stars in the last 24 hours, and 37 repos (4.5%) haven’t gained any new stars in the last week.\\nHere are examples of the growth trajectory of two of such repos compared to the growth curve of two more sustained software. Even though these two examples shown here are no longer used, I think they were valuable in showing the community what was possible, and it was cool that the authors were able to get things out so fast.\\n\\n\\n\\n\\n\\n\\nMy personal favorite ideas\\nSo many cool ideas are being developed by the community. Here are some of my favorites.\\n\\nBatch inference optimization: FlexGen, llama.cpp\\nFaster decoder with techniques such as Medusa, LookaheadDecoding\\nModel merging: mergekit\\nConstrained sampling: outlines, guidance, SGLang\\nSeemingly niche tools that solve one problem really well, such as einops and safetensors.\\n\\nConclusion\\nEven though I included only 845 repos in my analysis, I went through several thousands of repos. I found this helpful for me to get a big-picture view of the seemingly overwhelming AI ecosystem. I hope the list is useful for you too. Please do let me know what repos I’m missing, and I’ll add them to the list!\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPlease enable JavaScript to view the comments powered by Disqus.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nchip@[thiswebsite]\\n\\n\\n\\nchiphuyen\\n\\n\\n\\nchipro\\n\\n\\n\\nchipiscrazy\\n\\n\\n\\n\\n\\n\\n\\n\\nhuyenchip19\\n\\n\\nchiphuyen\\n\\n\\n\\n\\nI help companies deploy machine learning into production. I write about AI applications, tooling, and best practices.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\", metadata={'source': './data_source/ai-oss.html', 'title': 'What I learned from looking at 900 most popular open source AI tools'})]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "\n",
    "html_loader = BSHTMLLoader(html_path)\n",
    "docs = html_loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using bs4, the title will extracted into metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': './data_source/ai-oss.html',\n",
       " 'title': 'What I learned from looking at 900 most popular open source AI tools'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load from Markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='openai\\n\\nopenai-cookbook\\n\\nPublic\\n\\nNotifications\\n\\nFork\\n    8.8k\\n\\nStar\\n          55.7k\\n\\n\\n\\nCode\\n\\nIssues\\n          32\\n\\nPull requests\\n          40\\n\\nActions\\n\\nSecurity\\n\\nInsights\\n\\nCode\\n\\nIssues\\n\\nPull requests\\n\\nActions\\n\\nSecurity\\n\\nInsights', metadata={'source': './data_source/OpenAI_cookbook_README.md'})]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "markdown_file = \"./data_source/OpenAI_cookbook_README.md\"\n",
    "modes = [\"single\", \"elements\", \"paged\"]\n",
    "\n",
    "\"\"\"\n",
    "single: load content into a single document\n",
    "elements: split each element of markdown file into a document\n",
    "paged: each page is a document\n",
    "\"\"\"\n",
    "\n",
    "markdown_loader = UnstructuredMarkdownLoader(markdown_file, mode=modes[0])\n",
    "docs = markdown_loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Load from Web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"\\n\\n\\n\\nMultimodality and Large Multimodal Models (LMMs)\\n\\n\\n        \\n        Oct 10, 2023\\n      \\n      \\n        ‚Ä¢ Chip Huyen\\n\\n\\n\\nFor a long time, each ML model operated in one data mode ‚Äì text (translation, language modeling), image (object detection, image classification), or audio (speech recognition).\\nHowever, natural intelligence is not limited to just a single modality. Humans can read and write text. We can see images and watch videos. We listen to music to relax and watch out for strange noises to detect danger. Being able to work with multimodal data is essential for us or any AI to operate in the real world.\\nOpenAI noted in their GPT-4V system card that ‚Äúincorporating additional modalities (such as image inputs) into LLMs is viewed by some as a key frontier in AI research and development.‚Äù\\nIncorporating additional modalities to LLMs (Large Language Models) creates LMMs (Large Multimodal Models). In the last year, every week, a major research lab introduced a new LMM, e.g. DeepMind‚Äôs Flamingo, Salesforce‚Äôs BLIP, Microsoft‚Äôs KOSMOS-1, Google‚Äôs PaLM-E, and Tencent‚Äôs Macaw-LLM. Chatbots like ChatGPT and Gemini are LMMs.\\nNot all multimodal systems are LMMs. For example, text-to-image models like Midjourney, Stable Diffusion, and Dall-E are multimodal but don‚Äôt have a language model component. Multimodal can mean one or more of the following:\\n\\nInput and output are of different modalities (e.g. text-to-image, image-to-text)\\nInputs are multimodal (e.g. a system that can process both text and images)\\nOutputs are multimodal (e.g. a system that can generate both text and images)\\n\\nThis post covers multimodal systems in general, including LMMs. It consists of 3 parts.\\n\\nPart 1 covers the context for multimodality, including why multimodal, different data modalities, and types of multimodal tasks.\\nPart 2 discusses the fundamentals of a multimodal system, using the examples of CLIP, which lays the foundation for many future multimodal systems, and Flamingo, whose impressive performance gave rise to LMMs.\\nPart 3 discusses some active research areas for LMMs, including generating multimodal outputs and adapters for more efficient multimodal training, covering newer multimodal systems such as BLIP-2, LLaVA, LLaMA-Adapter V2, LAVIN, etc.\\n\\nThe post is long. Feel free to skip to the sections most interesting to you.\\n‚ö† Ambiguous terminology ‚ö†\\nMultimodal data can also refer to multimodal distributions, e.g. bimodal distribution, which is different from multimodal data in this post.\\n\\n\\nTable of contents\\nPart 1. Understanding Multimodal\\n‚Ä¶. Why multimodal\\n‚Ä¶. Data modalities\\n‚Ä¶. Multimodal tasks\\n‚Ä¶‚Ä¶.. Generation\\n‚Ä¶‚Ä¶.. Vision-language understanding\\nPart 2. Fundamentals of Multimodal Training\\n‚Ä¶. CLIP: Contrastive Language-Image Pre-training\\n‚Ä¶‚Ä¶.. CLIP‚Äôs high-level architecture\\n‚Ä¶‚Ä¶.. Natural language supervision\\n‚Ä¶‚Ä¶.. Contrastive learning\\n‚Ä¶‚Ä¶.. CLIP applications\\n‚Ä¶. Flamingo: the dawns of LMMs\\n‚Ä¶‚Ä¶.. Flamingo‚Äôs high-level architecture\\n‚Ä¶‚Ä¶.. Data\\n‚Ä¶‚Ä¶.. Flamingo‚Äôs vision encoder\\n‚Ä¶‚Ä¶.. Flamingo‚Äôs language model\\n‚Ä¶. TL;DR: CLIP vs. Flamingo\\nPart 3. Research Directions for LMMs\\n‚Ä¶. Incorporating more data modalities\\n‚Ä¶. Multimodal systems for instruction-following\\n‚Ä¶. Adapters for more efficient multimodal training\\n‚Ä¶. Generating multimodal outputs\\nConclusion\\nResources\\n\\n\\nPart 1. Understanding Multimodal\\nWhy multimodal\\nMany use cases are impossible without multimodality, especially those in industries that deal with a mixture of data modalities such as healthcare, robotics, e-commerce, retail, gaming, etc.\\n\\n\\n\\n\\n    An example of how multimodality can be used in healthcare. Image from Multimodal biomedical AI (Acosta et al., Nature Medicine 2022)\\n\\n\\nNot only that, incorporating data from other modalities can help boost model performance. Shouldn‚Äôt a model that can learn from both text and images perform better than a model that can learn from only text or only image?\\nMultimodal systems can provide a more flexible interface, allowing you to interact with them in whichever way works best for you at the moment. Imagine you can ask a question by typing, talking, or just pointing your camera at something.\\nOne use case that I‚Äôm especially excited about, is that multimodality can also enable visually impaired people to browse the Internet and also navigate the real world.\\n\\n\\n\\n\\n    Some cool multimodal use cases from GPT-4V\\n\\n\\nData modalities\\nDifferent data modes are text, image, audio, tabular data, etc. One data mode can be represented or approximated in another data mode. For example:\\n\\nAudio can be represented as images (mel spectrograms).\\nSpeech can be transcribed into text, though its text-only representation loses information such as volume, intonation, pauses, etc.\\nAn image can be represented as a vector, which, in turn, can be flattened and represented as a sequence of text tokens.\\nA video is a sequence of images plus audio. ML models today mostly treat videos as sequences of images. This is a severe limitation, as sounds have proved to be just as important as visuals for videos. 88% of TikTok users shared that sound is essential for their TikTok experience.\\nA text can be represented as an image if you simply take a picture of it.\\nA data table can be converted into a chart, which is an image.\\n\\n\\nHow about other data modalities?\\nAll digital data formats can be represented using bitstrings (strings of 0 and 1) or bytestrings. A model that can effectively learn from bitstrings or bytestrings will be very powerful, and it can learn from any data mode.\\nThere are other data modalities we haven‚Äôt touched on, such as graphs and 3D assets. We also haven‚Äôt touched on the formats used to represent smell and touch (haptics).\\n\\n\\nIn ML today, audio is still largely treated as a voice-based alternative to text. The most common use cases for audio are still speech recognition (speech-to-text) and speech synthesis (text-to-speech). Non-speech audio use cases, e.g. music generation, are still pretty niche. See the fake Drake & Weeknd song and MusicGen model on HuggingFace.\\n\\n\\n\\nImage is perhaps the most versatile format for model inputs, as it can be used to represent text, tabular data, audio, and to some extent, videos. There‚Äôs also so much more visual data than text data. We have phones/webcams that constantly take pictures and videos today.\\nText is a much more powerful mode for model outputs. A model that can generate images can only be used for image generation, whereas a model that can generate text can be used for many tasks: summarization, translation, reasoning, question answering, etc.\\nFor simplicity, we‚Äôll focus on 2 modalities: images and text. The learnings can be somewhat generalized to other modalities.\\nMultimodal tasks\\nTo understand multimodal systems, it‚Äôs helpful to look at the tasks they are built to solve. There are many tasks and many possible ways to organize them. In literature, I commonly see vision-language tasks divided into two groups: generation and vision-language understanding (VLU), which is the umbrella term for all tasks that don‚Äôt require generation. The line between these two groups is blurred, as being able to generate answers requires understanding too.\\nGeneration\\nFor generative tasks, the output can be unimodal (e.g. text, image, 3D rendering) or multimodal. While unimodal outputs are common today, multimodal outputs are still shaping up. We‚Äôll discuss multimodal outputs at the end of this post.\\nImage generation (text-to-image synthesis)\\nThis task category is straightforward. Examples: Dall-E, Stable Diffusion, and Midjourney.\\nText generation\\nA common text generation task is visual question answering. Instead of relying only on text for the context, you can give the model both text and images. Imagine you can point your camera to anything and ask questions like: ‚ÄúMy car won‚Äôt start. What‚Äôs wrong with it?‚Äù, ‚ÄúHow to make this dish?‚Äù, or ‚ÄúWhat is this meme about?‚Äù.\\nAnother common use case is image captioning, which can be used as part of a text-based image retrieval system. An organization might have millions, if not billions, of images: product images, graphs, designs, team pictures, promotional materials, etc. AI can automatically generate captions and metadata for them, making it easier to find the exact images you want.\\nVision-language understanding\\nWe‚Äôll zoom into two task types: classification and text-based image retrieval (TBIR).\\nClassification\\nClassification models can only generate outputs that belong to a pre-determined list of classes. This works when you only care about a fixed number of outcomes. For example, an OCR system only needs to predict if a visual is one of the known characters (e.g. a digit or a letter).\\nSide note: An OCR system processes data at the character level. When used together with a system that can understand the broader context, it can improve use cases such as allowing you to ‚Äútalk‚Äù to any textbook, contract, assembly instructions, etc.\\n\\n\\n\\n\\n    Document processing with GPT-4V. The model's mistake is highlighted in red.\\n\\n\\nOne related task to classification is image-to-text retrieval: given an image and a pool of pre-defined texts, find the text that‚Äôs most likely to accompany the image. This can be helpful for product image search, i.e. retrieving product reviews from a picture.\\nText-based image retrieval (image search)\\nImage search matters not only for search engines but also for enterprises to be able to search through all their internal images and documents. Some people call text-based image retrieval ‚Äútext-to-image retrieval‚Äù.\\nThere are several approaches to text-based image retrieval. Two of them are:\\n\\nGenerate captions and metadata for each image, either manually or automatically (see image captioning in Text generation). Given a text query, find images whose captions/metadata are closest to this text query.\\nTrain a joint embedding space for both images and text. Given a text query, generate an embedding for this query, and find all images whose embeddings are closest to this embedding.\\n\\nThe second approach is more flexible, and I believe will be more widely used. This approach requires having a strong joint embedding space for both vision and language, like the one that OpenAI‚Äôs CLIP developed.\\nPart 2. Fundamentals of Multimodal Training\\nGiven the existence of so many amazing multimodal systems, a challenge of writing this post is choosing which systems to focus on. In the end, I decided to focus on two models: CLIP (2021) and Flamingo (2022) both for their significance as well as availability and clarity of public details.\\n\\nCLIP was the first model that could generalize to multiple image classification tasks with zero- and few-shot learning.\\nFlamingo wasn‚Äôt the first large multimodal model that could generate open-ended responses (Salesforce‚Äôs BLIP came out 3 months prior). However, Flamingo‚Äôs strong performance prompted some to consider it the GPT-3 moment in the multimodal domain.\\n\\nEven though these two models are older, many techniques they use are still relevant today. I hope they serve as the foundation to understanding newer models. The multimodal space is evolving repaidly, with many new ideas being developed. We‚Äôll go over these newer models in Part 3.\\nAt a high level, a multimodal system consists of the following components:\\n\\nAn encoder for each data modality to generate the embeddings for data of that modality.\\nA way to align embeddings of different modalities into the same multimodal embedding space.\\n[Generative models only] A language model to generate text responses. Since inputs can contain both text and visuals, new techniques need to be developed to allow the language model to condition its responses on not just text, but also visuals.\\n\\nIdeally, as many of these components should be pretrained and reusable as possible.\\nCLIP: Contrastive Language-Image Pre-training\\nCLIP‚Äôs key contribution is its ability to map data of different modalities, text and images, into a shared embedding space. This shared multimodal embedding space makes text-to-image and image-to-text tasks so much easier.\\nTraining this multimodal embedding space also produced a strong image encoder, which allows CLIP to achieve competitive zero-shot performance on many image classification tasks. This strong image encoder can be used for many other tasks: image generation, visual question answering, and text-based image retrieval. Flamingo and LLaVa use CLIP as their image encoder. DALL-E uses CLIP to rerank generated images. It‚Äôs unclear if GPT-4V uses CLIP.\\n\\n\\n\\n\\n    Zero-shot image classification with CLIP\\n\\n\\nCLIP leveraged natural language supervision and contrastive learning, which allowed CLIP to both scale up their data and make training more efficient. We‚Äôll go over why/how these two techniques work.\\nCLIP's high-level architecture\\n\\n\\n\\n\\n    CLIP's architecture. Both encoders and projection matrices are jointly trained together from scratch. The training goal is to maximize the similarity scores of the right (image, text) pairings while minimizing the similarity scores of the wrong pairings (contrastive learning). \\n\\n\\nFor the image encoder, the authors experimented with both ResNet and ViT. Their best-performing model is ViT-L/14@336px:\\n\\nLarge vision transformer (ViT-L)\\n14 patches (each image is divided into 14 sub-images)\\non 336x336 pixel input\\n\\nFor the text encoder, CLIP uses a Transformer model similar to GPT-2 but smaller. Their base model has only 63M parameters with 8 attention heads. The authors found CLIP‚Äôs performance to be less sensitive to the capacity of the text encoder.\\nEmbeddings generated by the image encoder and text encoder are projected into the same embedding space using two projection matrices \\\\(W_v\\\\) and \\\\(W_l\\\\).\\n\\nGiven an image embedding \\\\(V_i\\\\), the corresponding multimodal embedding is computed as: \\\\(W_vV_i\\\\).\\nGiven a text embedding \\\\(L_i\\\\), the corresponding multimodal embedding is computed as: \\\\(W_lL_i\\\\).\\n\\nWhen people say CLIP embeddings, they either refer to these multimodal embeddings or the embeddings generated by CLIP‚Äôs image encoder.\\nNatural language supervision\\nFor many years, image models were trained with manually annotated (image, text) datasets (e.g. ImageNet, MS COCO). This isn‚Äôt scalable. Manual annotation is time-consuming and expensive.\\nThe CLIP paper noted that none of the then-available (image, text) datasets was big and high quality enough. They created their own dataset ‚Äì 400M (image, text) pairs ‚Äì as follows.\\n\\nConstruct a list of 500,000 queries. Queries are common words, bigrams, and titles of popular Wikipedia articles.\\nFind images matching these queries (string and substring match). The paper mentioned this search did NOT happen on search engines but didn‚Äôt specify where. My theory is that since OpenAI already scraped the entire Internet for their GPT models, they probably just queried their internal database.\\nEach image is paired with a text that co-occurs with it (e.g. captions, comments) instead of the query since queries are too short to be descriptive.\\n\\nBecause some queries are more popular than others, to avoid data imbalance, they used at most 20K images for a query.\\nContrastive learning\\nPre-CLIP, most vision-language models were trained using a classifier or language model objectives. Contrastive objective is a clever technique that allows CLIP to scale and generalize to multiple tasks.\\nWe‚Äôll show why the constrastive objective works better for CLIP using an example task of image captioning: given an image, generate a text that describes it.\\nClassifier objective\\nA classifier predicts the correct class among a predetermined list of classes. This works when the output space is finite. Previous models that work with (image, text) pair datasets all had this limitation. For example, models working with ILSVRC-2012 limited themselves to 1,000 classes, and JFT-300M to 18,291 classes.\\nThis objective limits not only the model‚Äôs capacity to output meaningful responses but also its capacity for zero-shot learning. Say, if the model was trained to predict among 10 classes, it won‚Äôt work for a task that has 100 classes.\\nLanguage model objective\\nIf a classifier outputs only one class for each input, a language model outputs a sequence of classes. Each generated class is called a token. Each token is from a predetermined list, the vocabulary, of the language model.\\n\\n\\n\\n\\n    Classifier vs. language model objectives\\n\\n\\nContrastive objective\\nWhile the language model objective allows for vastly more flexible outputs, CLIP authors noted this objective made the training difficult. They hypothesized that this is because the model tries to generate exactly the text accompanying each image, while many possible texts can accompany an image: alt-text, caption, comments, etc.\\nFor example, in the Flickr30K dataset, each image has 5 captions provided by human annotators, and the captions for the same image can be very different.\\n\\n\\n\\n\\n\\n\\nContrastive learning is to overcome this challenge. Instead of predicting the exact text of each image, CLIP was trained to predict whether a text is more likely to accompany an image than other texts.\\nFor each batch of \\\\(N\\\\) (image, text) pairs, the model generates N text embeddings and N image embeddings.\\n\\nLet \\\\(V_1, V_2, ..., V_n\\\\) be the embeddings for the \\\\(N\\\\) images.\\nLet \\\\(L_1, L_2, ..., L_n\\\\) be the embeddings for the \\\\(N\\\\) texts.\\n\\nCLIP computes the cosine similarity scores of the \\\\(N^2\\\\) possible (\\\\(V_i, L_j\\\\)) pairings. The model is trained to maximize the similarity scores of the \\\\(N\\\\) correct pairings while minimizing the scores of the \\\\(N^2 - N\\\\) incorrect pairings. For CLIP, \\\\(N = 32,768\\\\).\\n\\n\\n\\n\\n\\n\\nAnother way to look at this is that each training batch of CLIP is two classification tasks.\\n\\n\\nEach image can be paired with N possible texts, and the model tries to predict the correct one. This is the same setup as image-to-text retrieval.\\n\\n\\\\[L_{\\\\text{contrastive:txt2im}} = -\\\\frac{1}{N}\\\\sum_i^N\\\\log(\\\\frac{\\\\exp(L_i^TV_i\\\\beta)}{\\\\sum_j^N\\\\exp(L_i^TV_j\\\\beta)})\\\\]\\n  \\n\\nEach text can be paired with N possible images, and the model tries to predict the correct image. This is the same setup as text-to-image retrieval.\\n\\n\\\\[L_{\\\\text{contrastive:im2txt}} = -\\\\frac{1}{N}\\\\sum_i^N\\\\log(\\\\frac{\\\\exp(V_i^TL_i\\\\beta)}{\\\\sum_j^N\\\\exp(V_i^TL_j\\\\beta)})\\\\]\\n  \\n\\nThe sum of these two losses is minimized. \\uf8ffùõΩ is a trainable inverse temperature parameter.\\nThis is what it all looks like in pseudocode.\\n\\n\\n\\n\\n\\nCLIP authors found that the contrastive objective provided a 12x improvement in efficiency compared to the language model objective baseline while producing higher-quality image embeddings.\\n\\n\\n\\n\\n\\nCLIP applications\\nClassification\\nToday, for many image classification tasks, CLIP is still a strong out-of-the-box baseline to be used as-is or fine-tuned.\\n\\n\\n\\n\\n\\nText-based image retrieval\\nSince CLIP‚Äôs training process was conceptually similar to image-to-text retrieval and text-to-image retrieval, CLIP ‚Äúdisplays significant promise for widely-applicable tasks like image retrieval or search.‚Äù However, ‚Äúon image retrieval, CLIP‚Äôs performance relative to the overall state of the art is noticeably lower.‚Äù\\nThere are attempts to use CLIP for image retrieval. For example, clip-retrieval package works as follows:\\n\\nGenerate CLIP embeddings for all your images and store them in a vector database.\\nFor each text query, generate a CLIP embedding for this text.\\nQuery in the vector database for all images whose embeddings are close to this text query embedding.\\n\\nImage generation\\nCLIP‚Äôs joint image-text embeddings are useful for image generation. Given a text prompt, DALL-E (2021) generates many different visuals and uses CLIP to rerank these visuals before showing the top visuals to users.\\nIn 2022, OpenAI introduced unCLIP, a text-to-image synthesis model conditioned on CLIP latents. It consists of two main components:\\n\\nCLIP is trained and frozen. The pretrained CLIP model can generate embeddings for both text and images in the same embedding space.\\nTwo things happen at image generation:\\n    \\nUse CLIP to generate embedding for this text.\\nUse a diffusion decoder to generate images conditioned on this embedding.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nText generation: visual question answering, captioning\\nCLIP authors did attempt to create a model for text generation. One version they experimented with is called LM RN50. Though this model could generate text responses, its performance was consistently around 10% below CLIP‚Äôs best-performing model on all the vision-language understanding tasks that CLIP was evaluated on.\\nWhile today CLIP isn‚Äôt used directly for text generation, its image encoder is often the backbone for LMMs that can generate texts.\\nFlamingo: the dawns of LMMs\\nUnlike CLIP, Flamingo can generate text responses. In a reductive view, Flamingo is CLIP + a language model, with added techniques to make it possible for the language model to generate text tokens conditioned on both visual and text inputs.\\n\\n\\n\\n\\n    Flamingo can generate text responses conditioned on both text and images\\n\\n\\nFlamingo's high-level architecture\\nAt a high level, Flamingo consists of 2 parts:\\n\\nVision encoder: a CLIP-like model is trained using contrastive learning. The text encoder of this model is then discarded. The vision encoder is frozen to be used in the main model.\\nLanguage model: Flamingo finetunes Chinchilla to generate text tokens, conditioned on visuals and text, using language model loss, with two additional components Perceiver Resampler and GATED XATTN-DENSE layers. We‚Äôll discuss them later in this blog.\\n\\n\\n\\n\\n\\n\\n\\nData\\nFlamingo used 4 datasets: 2 (image, text) pair datasets, 1 (video, text) pair dataset, and 1 interleaved image and text dataset.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nDataset\\n\\nType\\n\\nSize\\n\\nHow\\n\\nTraining weight\\n\\n\\n\\nM3W\\n   \\nInterleaved image and text dataset\\n   \\n43M webpages\\n   \\nFor each webpage, they sample a random subsequence of 256 tokens and take up to the first 5 images included in the sampled sequence.\\n   \\n1.0\\n   \\n\\n\\nALIGN\\n   \\n(Image, text) pairs\\n   \\n1.8B pairs\\n   \\nTexts are alt-texts, averaging 12 tokens/text.\\n   \\n0.2\\n   \\n\\n\\nLTIP\\n   \\n(Image, text) pairs\\n   \\n312M pairs\\n   \\nTexts are long descriptions, averaging 20.5 tokens/text.\\n   \\n0.2\\n   \\n\\n\\nVTP\\n   \\n(Video, text) pairs\\n   \\n27M short videos\\n   \\n~22 seconds/video on average\\n   \\n0.03\\n   \\n\\n\\n\\nFlamingo's vision encoder\\nFlamingo first trains a CLIP-like model from scratch using contrastive learning. This component only uses the 2 (image, text) pair datasets, ALIGN and LTIP, totaling 2.1M (image, text) pairs. This is 5x larger than the dataset CLIP was trained on.\\n\\nFor the text encoder, Flamingo uses BERT instead of GPT-2.\\nFor the vision encoder, Flamingo uses a NormalizerFree ResNet (NFNet) F6 model.\\nText and vision embeddings are meanpooled before being projected to the joint embedding space.\\n\\nFlamingo's language model\\nFlamingo uses Chinchilla as their language model. More specifically, they freeze the 9 pretrained Chinchilla LM layers. A traditional language model predicts the next text token based on the preceding text tokens. Flamingo predicts the next text token based on both the preceding text and visual tokens.\\n\\n\\n\\n\\n    Next token generation is conditioned on both text and visual tokens. Illustration taken from Chunyuan Li's CVPR 2023 tutorial: Large Multimodal Models.\\n\\n\\nTo be able to generate text conditioned on both text and visual inputs, Flamingo relied on Perceiver Resampler and GATED XATTN-DENSE layers.\\nPerceiver Resampler\\nAs the visual inputs can be both images and videos, the vision encoder can produce a variable number of image or video features. Perceiver Resampler converts these variable features into a consistent 64 visual outputs.\\nInterestingly enough, while training the vision encoder, the resolution used was 288 x 288. However, at this phase, visual inputs are resized to 320 √ó 320. It‚Äôs been shown that a higher test-time resolution can lead to improved performance when using CNNs.\\n\\n\\n\\n\\n\\n\\nGATED XATTN-DENSE layers\\nGATED XATTN-DENSE layers are inserted between existing and frozen LM layers to allow the language model to attend more efficiently to the visual tokens when generating text tokens. Without these layers, Flamingo authors noted a drop of 4.2% in the overall score.\\n\\n\\n\\n\\n\\n\\nLoss function\\nFlamingo computes the likelihood of text \\\\(y\\\\) conditioned on the interleaved images and videos \\\\(x\\\\).\\n\\n\\\\[p(y|x) = \\\\prod_{l=1}^N p(y_l|y_{<l}, x_{\\\\leq l})\\\\]\\n\\nThe training loss function was a weighted sum of expected negative log-likelihoods of generated text across all 4 datasets, with \\\\(\\\\lambda_m\\\\) being the training weight of dataset \\\\(m\\\\).\\n\\n\\\\[\\\\sum_{m=1}^M \\\\lambda_m E_{(x, y)\\\\sim D_m} [ -\\\\sum_{l=1}^L \\\\log p(y|x)]\\\\]\\n\\nTraining\\nWhile the Chinchilla LM layers are finetuned and frozen, the additional components are trained from scratch, using all 4 Flamingo datasets, with different weights. Finding the right per-dataset weights was key to performance. The weight for each dataset is in the Training weight column in the dataset table above.\\nVTP‚Äôs weight is much smaller than other datasets (0.03 compared to 0.2 and 1), so its contribution to the training should be minimal. However, the authors noted that removing this dataset negatively affects performance on all video tasks.\\nWhile Flamingo isn‚Äôt open-sourced, there are many open-source replications of Flamingo.\\n\\nIDEFICS (HuggingFace)\\nmlfoundations/open_flamingo\\n\\nTL;DR: CLIP vs. Flamingo\\n\\n\\n\\n\\n\\n\\nPart 3. Research Directions for LMMs\\nCLIP is 3 years old and Flamingo is almost 2. While their architectures serve as a good foundation for us to understand how LMMs are built, there have been many new progresses in the space.\\nHere are a few directions that I‚Äôm excited about. This is far from an exhaustive list, both because this post has been long and because I‚Äôm still learning about the space too. If you have any pointers or suggestions, please let me know!\\nIncorporating more data modalities\\nToday, most multimodal systems work with text and images. It‚Äôs only a matter of time before we need systems that can incorporate other modalities such as videos, music, and 3D. Wouldn‚Äôt it be amazing to have one shared embedding space for ALL data modalities?\\nExamples of works in this space:\\n\\nULIP: Learning a Unified Representation of Language, Images, and Point Clouds for 3D Understanding (Xue et al., Dec 2022)\\nImageBind: One Embedding Space To Bind Them All (Girdhar et al., May 2023)\\nNExT-GPT: Any-to-Any Multimodal Large Language Model (Wu et al., Sep 2023)\\nJeff Dean‚Äôs ambitious Pathways project (2021): its vision is to ‚Äúenable multimodal models that encompass vision, auditory, and language understanding simultaneously.‚Äù\\n\\n\\n\\n\\n\\n\\nMultimodal systems for instruction-following\\nFlamingo was trained for completion, but not for dialogue or for following instructions. (If you‚Äôre not familiar with completion vs. dialogue, check out my post on RLHF). Many people are working on building LMMs that can follow instructions and have conversations, such as:\\n\\nMultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning (Xu et al., Dec 2022)\\nLLaVA: Visual Instruction Tuning (Liu et al., Apr 28, 2023)\\nInstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning (Salesforce, May 11, 2023)\\nLaVIN: Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models (Luo et al., May 24, 2023)\\n\\n\\n\\n\\n\\n    Examples of LaVIN's outputs compared to other LMMs, shown in LaVIN's paper\\n\\n\\nAdapters for more efficient multimodal training\\nWhile Flamingo used 9 pretrained and frozen layers from Chinchilla, it had to pretrain its vision encoder, Perceiver Resampler, and GATED XATTN-DENSE layers from scratch. These train-from-scratch modules could be compute-intensive. Many works focus on more efficient ways to bootstrap multimodal systems using less training from scratch.\\nSome works are quite promising. BLIP-2, for example, outperformed Flamingo-80B by 8.7% on zero-shot VQA-v2 with 54x fewer trainable parameters.\\nWorks in this space include:\\n\\nBLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\\n[LAVIN] Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models\\nLLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model\\n\\nThe two images below are from Chunyuan Li‚Äôs Large Multimodal Models tutorial at CVPR 2023, which is, btw, an excellent tutorial.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nGenerating multimodal outputs\\nWhile models that can process multimodal inputs are becoming the norm, multimodal output is still lagging. Many use cases require multimodal outputs. For example, if we ask ChatGPT to explain RLHF, an effective explanation might require graphs, equations, and even simple animations.\\nTo generate multimodal outputs, a model would first need to generate a shared intermediate output. One key question is what the intermediate output would look like.\\nOne option for intermediate output is text, which will then be used to generate/synthesize other actions.\\nFor example, CM3 (Aghajanyan et al., 2022) outputs HTML markup which can be compiled into webpages that contain not only text but also formattings, links, and images. GPT-4V generates Latex code, which can then be reconstructed as data tables.\\n\\n\\n\\n\\n    Sampled outputs from CM3\\n\\n\\n\\n\\n\\n\\n    GPT-4V generates Latex code, which can then be reconstructed as a data table\\n\\n\\nAnother option for intermediate output would be multimodal tokens. This is the option that Caiming Xiong, whose team at Salesforce has done a lot of awesome work on multimodality, told me. Each token will have a tag to denote whether it‚Äôs a text token or an image token. Image tokens will then be input into an image model like Diffusion to generate images. Text tokens will then be input into a language model.\\nGenerating Images with Multimodal Language Models (Koh et al., Jun 2023) is an awesome paper that shows how LMMs can generate and retrieve images together with generating texts. See below.\\n\\n\\n\\n\\n\\n\\n\\nConclusion\\nIt‚Äôs been a lot of fun going over so many multimodal papers as well as talking to people doing awesome work and trying to summarize the key patterns in one blog post. There‚Äôs so much about multimodality that I‚Äôm sure there are many things that I‚Äôve missed, but I hope that this post provides the core patterns that will help you develop multimodal systems and apply them to your work.\\nAs you see in part 3 of this post, we‚Äôre still in the early days of multimodal systems (so early that a friend told me he‚Äôs not sure if the LMM abbreviation would catch on). Yes, in most of my conversations, there‚Äôs little doubt that multimodal systems in general, and LMMs in particular, will be even more impactful than large language models. However, keep in mind that LMMs do not make LLMs obsolete. As LMMs extend upon LLMs, the performance of an LMM relies on the performance of its base LLM. Many labs that work on multimodal systems work on LLMs in parallel.\\nEarly reviewers\\nI‚Äôd like to thank the amazing early reviewers who gave me plenty of pointers and suggestions to make this post better: Han-chung Lee, Sam Reiswig, and Luke Metz.\\nResources\\nModels\\nAn incomplete list of multimodal systems by time to give you a sense of how fast the space is moving!\\n\\nMicrosoft COCO Captions: Data Collection and Evaluation Server (Apr 2015)\\nVQA: Visual Question Answering (May 2015)\\nVideoBERT: A Joint Model for Video and Language Representation Learning (Google, Apr 3, 2019)\\nLXMERT: Learning Cross-Modality Encoder Representations from Transformers (UNC Chapel Hill, Aug 20, 2019)\\n[CLIP] Learning Transferable Visual Models From Natural Language Supervision (OpenAI, 2021)\\nUnifying Vision-and-Language Tasks via Text Generation (UNC Chapel Hill, May 2021)\\nBLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation (Salesforce, Jan 28, 2022)\\nFlamingo: a Visual Language Model for Few-Shot Learning (DeepMind, April 29, 2022)\\nGIT: A Generative Image-to-text Transformer for Vision and Language (Microsoft, May 2, 2022)\\nMultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning (Xu et al., Dec 2022)\\nBLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models (Salesforce, Jan 30, 2023)\\nCross-Modal Fine-Tuning: Align then Refine (Shen et al., Feb 11, 2023)\\nKOSMOS-1: Language Is Not All You Need: Aligning Perception with Language Models (Microsoft, Feb 27, 2023)\\nPaLM-E: An Embodied Multimodal Language Model (Google, Mar 10, 2023)\\nLLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention (Zhang et al., Mar 28, 2023)\\nmPLUG-Owl: Modularization Empowers Large Language Models with Multimodality (Ye et al., Apr 2, 2023)\\nLLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model (Gao et al., Apr 28, 2023)\\nLLaVA: Visual Instruction Tuning (Liu et al., Apr 28, 2023)\\nX-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages (Chen et al., May 7, 2023)\\nInstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning (Salesforce, May 11, 2023)\\nTowards Expert-Level Medical Question Answering with Large Language Models (Singhal et al., May 16, 2023)\\nCheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models (Luo et al., May 24, 2023)\\nShikra: Unleashing Multimodal LLM‚Äôs Referential Dialogue Magic (SenseTime, Jun 3, 2023)\\nMacaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration (Tencent, Jun 15, 2023)\\n\\nOther resources\\n\\n[CVPR2023 Tutorial Talk] Large Multimodal Models: Towards Building and Surpassing Multimodal GPT-4\\n\\nSlides: Large Multimodal Models\\n\\n\\n[CMU course] 11-777 MMML\\n[Open source] Salesforce‚Äôs LAVIS\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPlease enable JavaScript to view the comments powered by Disqus.\\n\\n\\n\", metadata={'source': 'https://huyenchip.com/2023/10/10/multimodal.html'})]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "\n",
    "web_paths = [\"https://huyenchip.com/2023/10/10/multimodal.html\"]\n",
    "\n",
    "classes = ['post-content', 'post-title', 'post-header', 'page-content']\n",
    "bs4_strainer = bs4.SoupStrainer(class_=classes)\n",
    "\n",
    "web_loader = WebBaseLoader(\n",
    "    web_paths=web_paths,\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4_strainer\n",
    "    ),\n",
    ")\n",
    "docs = web_loader.load()\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "online_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
